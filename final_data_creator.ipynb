{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2133,\n",
       " 1457,\n",
       " 986,\n",
       " 90,\n",
       " 3,\n",
       " 15.958274730426629,\n",
       " [11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  14,\n",
       "  7,\n",
       "  11,\n",
       "  36,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  8,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  13,\n",
       "  36,\n",
       "  17,\n",
       "  23,\n",
       "  17,\n",
       "  17,\n",
       "  18,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  25,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  12,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  20,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  9,\n",
       "  11,\n",
       "  14,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  14,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  8,\n",
       "  6,\n",
       "  13,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  28,\n",
       "  15,\n",
       "  23,\n",
       "  15,\n",
       "  15,\n",
       "  20,\n",
       "  22,\n",
       "  15,\n",
       "  17,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  11,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  8,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  11,\n",
       "  15,\n",
       "  19,\n",
       "  31,\n",
       "  21,\n",
       "  22,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  11,\n",
       "  8,\n",
       "  21,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  15,\n",
       "  16,\n",
       "  5,\n",
       "  14,\n",
       "  22,\n",
       "  18,\n",
       "  11,\n",
       "  11,\n",
       "  47,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  42,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  11,\n",
       "  11,\n",
       "  22,\n",
       "  15,\n",
       "  26,\n",
       "  18,\n",
       "  11,\n",
       "  11,\n",
       "  20,\n",
       "  15,\n",
       "  15,\n",
       "  12,\n",
       "  5,\n",
       "  55,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  21,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  12,\n",
       "  15,\n",
       "  37,\n",
       "  16,\n",
       "  17,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  11,\n",
       "  43,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  13,\n",
       "  6,\n",
       "  14,\n",
       "  26,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  12,\n",
       "  5,\n",
       "  7,\n",
       "  14,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  12,\n",
       "  5,\n",
       "  9,\n",
       "  22,\n",
       "  17,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  19,\n",
       "  15,\n",
       "  23,\n",
       "  16,\n",
       "  31,\n",
       "  16,\n",
       "  19,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  8,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  14,\n",
       "  7,\n",
       "  18,\n",
       "  17,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  29,\n",
       "  15,\n",
       "  15,\n",
       "  20,\n",
       "  15,\n",
       "  20,\n",
       "  15,\n",
       "  14,\n",
       "  7,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  11,\n",
       "  12,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  21,\n",
       "  13,\n",
       "  6,\n",
       "  19,\n",
       "  15,\n",
       "  15,\n",
       "  18,\n",
       "  15,\n",
       "  20,\n",
       "  11,\n",
       "  22,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  39,\n",
       "  18,\n",
       "  17,\n",
       "  24,\n",
       "  18,\n",
       "  17,\n",
       "  17,\n",
       "  22,\n",
       "  17,\n",
       "  33,\n",
       "  18,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  13,\n",
       "  36,\n",
       "  24,\n",
       "  17,\n",
       "  9,\n",
       "  34,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  23,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  14,\n",
       "  7,\n",
       "  30,\n",
       "  5,\n",
       "  26,\n",
       "  19,\n",
       "  15,\n",
       "  15,\n",
       "  22,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  26,\n",
       "  15,\n",
       "  15,\n",
       "  14,\n",
       "  7,\n",
       "  11,\n",
       "  15,\n",
       "  22,\n",
       "  17,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  21,\n",
       "  15,\n",
       "  27,\n",
       "  17,\n",
       "  22,\n",
       "  15,\n",
       "  8,\n",
       "  22,\n",
       "  17,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  8,\n",
       "  10,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  19,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  9,\n",
       "  17,\n",
       "  17,\n",
       "  25,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  28,\n",
       "  24,\n",
       "  33,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  19,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  13,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  20,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  26,\n",
       "  20,\n",
       "  15,\n",
       "  8,\n",
       "  36,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  14,\n",
       "  7,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  29,\n",
       "  15,\n",
       "  22,\n",
       "  15,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  12,\n",
       "  5,\n",
       "  12,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  13,\n",
       "  6,\n",
       "  21,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  8,\n",
       "  12,\n",
       "  15,\n",
       "  15,\n",
       "  26,\n",
       "  15,\n",
       "  15,\n",
       "  17,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  19,\n",
       "  12,\n",
       "  5,\n",
       "  13,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  11,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  14,\n",
       "  7,\n",
       "  67,\n",
       "  19,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  20,\n",
       "  5,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  13,\n",
       "  6,\n",
       "  10,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  8,\n",
       "  12,\n",
       "  5,\n",
       "  17,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  14,\n",
       "  7,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  22,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  21,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  13,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  22,\n",
       "  17,\n",
       "  21,\n",
       "  24,\n",
       "  17,\n",
       "  31,\n",
       "  38,\n",
       "  17,\n",
       "  33,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  23,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  10,\n",
       "  11,\n",
       "  31,\n",
       "  24,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  9,\n",
       "  11,\n",
       "  23,\n",
       "  30,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  9,\n",
       "  10,\n",
       "  22,\n",
       "  19,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  14,\n",
       "  7,\n",
       "  17,\n",
       "  5,\n",
       "  15,\n",
       "  16,\n",
       "  14,\n",
       "  7,\n",
       "  11,\n",
       "  19,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  21,\n",
       "  17,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  18,\n",
       "  15,\n",
       "  8,\n",
       "  14,\n",
       "  22,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  13,\n",
       "  6,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  28,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  12,\n",
       "  5,\n",
       "  16,\n",
       "  15,\n",
       "  23,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  8,\n",
       "  13,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  21,\n",
       "  28,\n",
       "  28,\n",
       "  17,\n",
       "  34,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  22,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  14,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  28,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  9,\n",
       "  4,\n",
       "  30,\n",
       "  13,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  13,\n",
       "  6,\n",
       "  11,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  8,\n",
       "  9,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  12,\n",
       "  5,\n",
       "  12,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  23,\n",
       "  15,\n",
       "  13,\n",
       "  6,\n",
       "  28,\n",
       "  29,\n",
       "  17,\n",
       "  15,\n",
       "  15,\n",
       "  21,\n",
       "  21,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  8,\n",
       "  11,\n",
       "  15,\n",
       "  33,\n",
       "  16,\n",
       "  22,\n",
       "  15,\n",
       "  15,\n",
       "  21,\n",
       "  23,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  9,\n",
       "  14,\n",
       "  22,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  13,\n",
       "  6,\n",
       "  8,\n",
       "  5,\n",
       "  11,\n",
       "  20,\n",
       "  21,\n",
       "  15,\n",
       "  22,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  23,\n",
       "  7,\n",
       "  27,\n",
       "  15,\n",
       "  21,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  13,\n",
       "  6,\n",
       "  11,\n",
       "  41,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  17,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  11,\n",
       "  36,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  8,\n",
       "  23,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  11,\n",
       "  26,\n",
       "  16,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  10,\n",
       "  5,\n",
       "  13,\n",
       "  15,\n",
       "  18,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  13,\n",
       "  6,\n",
       "  9,\n",
       "  15,\n",
       "  15,\n",
       "  ...])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NER 前處理(長度限制) new share\n",
    "# NES = not entity sentence\n",
    "# Fix Length = FL\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import os, csv\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "model=\"w2ner\"\n",
    "data_format = \"test\"        #dev, train/test\n",
    "file_format = \"cadec\"       #share13/share14/cadec\n",
    "ans_data_path = \"test\"      #dev, train /test\n",
    "\n",
    "#share13\n",
    "# from_enhance_count =   \"model_share14_new_split_ori_our_area\"\n",
    "# from_enhance_count =  \"enhance_model_share14_our_split_12_look_both_side_ES-dynamic_size-F3-B3\"\n",
    "# from_enhance_count = \"enhance_model_share14_our_split_14_inner_look_both_side_ES-dynamic_size-F3-B3\"\n",
    "# model_share14_new_split_ori_our_area/model_share14_new_split_ori_dai_area\n",
    "to_enhance_count = \"enhance_model_cadec_our_split_1_new_inner\"\n",
    "split_type=\"our\"\n",
    "#enhance_share13_new_origin_1/enhance_share13_share13_origin_1\n",
    "model=\"w2ner\"\n",
    "if \"cadec\" in file_format:\n",
    "    data_format = data_format        #dev, train/test\n",
    "    file_format = 'cadec'      #share13/share14/cadec\n",
    "    ans_data_path = data_format      #dev, train /test\n",
    "    answer_file='acl/cadec/ann'\n",
    "    folder_path = 'acl/'+file_format+'/text'\n",
    "    # path = \"acl/data/cadec/new_\"+file_format+\"/\"+file_format+\"_\"+data_format+\".json\"\n",
    "    path = \"acl/\"+file_format+\"/\"+file_format+\"_word_list.json\"\n",
    "    pattern = r'[\\w]+|[^\\s\\w]'\n",
    "    pattern_s = r'[\\w]+|\\n|[\\s]+|[^\\s\\w]'\n",
    "else:\n",
    "    data_format = data_format        #dev, train/test\n",
    "    if \"share13\" in file_format:\n",
    "        file_format = 'share13'       #share13/share14/cadec\n",
    "    else:\n",
    "        file_format = 'share14'      #share13/share14/cadec\n",
    "\n",
    "    if ans_data_path==\"test\":\n",
    "        ans_data_path=ans_data_path\n",
    "    else:\n",
    "        ans_data_path=\"train\"\n",
    "    answer_file='D:/hw/W2NER-final/acl/data/'+file_format+'/'+ans_data_path+'.ann'\n",
    "    folder_path = 'D:/hw/W2NER-final/acl/data/'+file_format+'/'+ans_data_path+'/text'\n",
    "    if \"our\" in split_type:\n",
    "        # our pattern \n",
    "        pattern = r'[\\w]+|[^\\s\\w]'\n",
    "        pattern_s = r'[\\w]+|\\n|[\\s]+|[^\\s\\w]'\n",
    "        # path = \"D:/hw/W2NER-final/acl/data/cadec/new_\"+file_format+\"/\"+file_format+\"_\"+data_format+\".json\"\n",
    "        path = \"acl/\"+file_format+\"/\"+file_format+\"_word_list.json\"\n",
    "    if \"dai\" in split_type:\n",
    "        # new pattern - dai\n",
    "        contractions = set([\"n't\", \"'s\", \"'ve\", \"'re\", \"'ll\", \"'d\", \"'m\"])\n",
    "        contractions |= set([x.replace(\"'\", \"’\") for x in contractions])\n",
    "\n",
    "        contractions_pattern_1 = \"\\w+(?=\"+\")|\\w+(?=\".join(list(contractions))+\")\"\n",
    "        contractions_pattern_2 = \"|\".join(list(contractions))\n",
    "\n",
    "        pattern = \"\\w+(?=n't)|~\\w+|\"+contractions_pattern_1+\"|\\w+#\\w*|\\w+=\\w*|\\w+‘\\w*|\\w+“\\w*|\"+contractions_pattern_2+\"|\"+r\"\"\"[\\w\\d]+[*]+|[|]+|\\w+\"\\w+|\\w+'\\w+|[\\w]+|[^\\s\\w]\"\"\"\n",
    "        pattern_s = \"\\w+(?=n't)|~\\w+|\"+contractions_pattern_1+\"|\\w+#\\w*|\\w+=\\w*|\\w+‘\\w*|\\w+“\\w*|\"+contractions_pattern_2+\"|\"+r\"\"\"[\\w\\d]+[*]+|[|]+|\\w+\"\\w+|\\w+'\\w+|[\\w]+|[^\\s\\w]\"\"\"+\"|\\n|[\\s]+\"\n",
    "        path = \"D:/hw/W2NER-final/acl/data/cadec/new_\"+file_format+\"/\"+file_format+\"_\"+data_format+\"_new.json\"\n",
    "    \n",
    "\n",
    "\n",
    "ann_own_path = \"ann\"  #ann_vik / ann\n",
    "\n",
    "\n",
    "# #cadec\n",
    "# answer_file='D:/acl/data/cadec/adr/ann'\n",
    "#share13/14\n",
    "ans = pd.read_csv(answer_file, names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "ans_tmp = ans.groupby(['file_id'])\n",
    "sentence_length = []\n",
    "enhance_sentence_length = []\n",
    "\n",
    "# use ground true answer\n",
    "# ans_split = pd.read_csv(answer_file, names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, quoting=csv.QUOTE_NONE, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "\n",
    "# use predict answer\n",
    "# ans_split = pd.read_csv('ann/'+file_format+'/'+ann_own_path+'/w2ner_'+data_format+\"_\"+from_enhance_count+'.ann', names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, quoting=csv.QUOTE_NONE, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "\n",
    "# ans_split = pd.read_csv('D:/hw/W2NER-final/ann/share13_our_split_12_look_both_side_ES-dynamic_size-F4-B4/test/w2ner_test_0.ann', names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, quoting=csv.QUOTE_NONE, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "# ans_split = pd.read_csv(r\"D:\\hw\\TOE-final\\ann\\cadec_19\\test\\w2ner_test_0.ann\", names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\",\"entity_bound\"], keep_default_na=False, quoting=csv.QUOTE_NONE, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "\n",
    "# D:\\hw\\W2NER-final\\ann\\share13_our_split_12_look_both_side_ES-dynamic_size-F4-B4\\train\\w2ner_train_0.ann\n",
    "# ans_split_tmp = ans_split.groupby(['file_id'])\n",
    "# print('D:/hw/W2NER-main/ann/'+file_format+'/'+ann_own_path+'/w2ner_'+data_format+\"_\"+from_enhance_count+'.ann')\n",
    "\n",
    "ans_split = pd.DataFrame()\n",
    "# file_list = os.listdir(\"ann/model_share13_our_split_1_look_both_side_ES-dynamic_size-F4-B4/test\")\n",
    "file_list=[\"toe_test_0.ann\",\"toe_test_1.ann\"]\n",
    "# ,\"toe_test_1.ann\",\"toe_test_2.ann\"\n",
    "for enhance_file in file_list:\n",
    "    tmp_df = pd.read_csv(\"ann/cadec_19/\"+data_format+\"/\"+enhance_file, names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "    tmp_df['entity_bound'] = tmp_df['start_end'].apply(lambda x: \",\".join([x.split(',')[0],x.split(',')[-1]]))\n",
    "    tmp_df = tmp_df.drop_duplicates(subset=['file_id','entity_bound'])\n",
    "    ans_split = pd.concat([ans_split,tmp_df])\n",
    "\n",
    "ans_split_df_duplicated = ans_split.duplicated(subset=['file_id','entity_bound'], keep=False)\n",
    "# ans_split = ans_split[ans_split_df_duplicated].groupby(['file_id','entity_bound']).filter(lambda x: len(x) >= (len(file_list))*0.75)\n",
    "ans_split = ans_split[ans_split_df_duplicated].groupby(['file_id','entity_bound']).filter(lambda x: len(x) == (len(file_list)))\n",
    "ans_split = ans_split.drop_duplicates(subset=['file_id','entity_bound'])\n",
    "\n",
    "# # ans_split = p_c.drop_duplicates(subset=['file_id','start_end'])\n",
    "ans_split_tmp = ans_split.groupby(['file_id'])\n",
    "\n",
    "# ans_split.head()\n",
    "# 7896 7909\n",
    "# 設定資料夾路徑\n",
    "#cadec\n",
    "# folder_path = 'D:/acl/data/'+file_format+'/adr/text'\n",
    "#share13/share14\n",
    "\n",
    "# 設定輸出 JSONL 檔案的路徑\n",
    "# D:\\acl\\data\\cadec\\new_share13\\ann_to_share13\n",
    "# output_path = \"D:/acl/data/cadec/new_\"+file_format+\"/\"+ann_own_path+\"_to_\"+file_format+\"/toe_\"+file_format+\"_\"+data_format+\"_\"+to_enhance_count+\"_\"+output_format+\".json\"\n",
    "main_block_size =0\n",
    "ES_action_state  = 1\n",
    "NES_action_state = 1\n",
    "is_write =       0      # 1寫入        / 0 不寫\n",
    "get_all_entity = 1     # 1取得全部實體 / 0 則否\n",
    "total_sentence = 0 \n",
    "add_train_random = 0            #train random 0/1 沒有/有\n",
    "contain_merge_conflict_span = 0 # 0 衝突區間個體/ 1 衝突區間全部\n",
    "\n",
    "'''\n",
    "action_state\n",
    "{\n",
    "    0: not work\n",
    "    1: work\n",
    "}\n",
    "'''\n",
    "support_strategy = 2\n",
    "'''\n",
    "support_strategy\n",
    "{\n",
    "    0: look forward\n",
    "    1: look backward\n",
    "    2: look both side\n",
    "}\n",
    "'''\n",
    "look_forward_step = 4\n",
    "look_backward_step= 4\n",
    "\n",
    "# to_enhance_count\n",
    "if main_block_size==0:\n",
    "    main_block_mode = \"dynamic_size\"\n",
    "else:\n",
    "    main_block_mode = str(main_block_size)\n",
    "# to_enhance_count\n",
    "if support_strategy==0 and (ES_action_state or NES_action_state):\n",
    "    to_enhance_count += \"_look_forward\"\n",
    "    domain_t = main_block_mode+\"-\"+\"F\"+str(look_forward_step)+\"-\"+\"B0\"\n",
    "elif support_strategy==1 and (ES_action_state or NES_action_state):\n",
    "    to_enhance_count += \"_look_backward\"\n",
    "    domain_t = main_block_mode+\"-\"+\"F0\"+\"-\"+\"B\"+str(look_backward_step)\n",
    "elif support_strategy==2 and (ES_action_state or NES_action_state):\n",
    "    to_enhance_count += \"_look_both_side\"\n",
    "    domain_t = main_block_mode+\"-\"+\"F\"+str(look_forward_step)+\"-\"+\"B\"+str(look_backward_step)\n",
    "else:\n",
    "    to_enhance_count += \"\"\n",
    "# 7802 0  7882 1  7930 3\n",
    "\n",
    "if ES_action_state + NES_action_state==2:\n",
    "    domain_s = \"_ALL\"+\"-\"+domain_t\n",
    "elif ES_action_state==1:\n",
    "    domain_s = \"_ES\"+\"-\"+domain_t\n",
    "elif NES_action_state==1:\n",
    "    domain_s = \"_NES\"+\"-\"+domain_t\n",
    "else:\n",
    "    domain_s = \"_None\"\n",
    "\n",
    "# output_path = \"D:/hw/W2NER-final/enhance_data/\"+file_format+\"/\"+model+\"_\"+data_format+\"_\"+to_enhance_count+domain_s+\".json\"\n",
    "output_path = r\"D:\\hw\\W2NER-final\\enhance_data\\cadec_3\\w2ner_test_enhance_0_look_both_side_ALL-dynamic_size-F4-B5.json\"    \n",
    "# print(output_path)\n",
    "\n",
    "# \n",
    "merge_intervals_limit = 5 # char 數為單位\n",
    "entity_intervallimit = 20       # 設35會錯\n",
    "# 10*5\n",
    "total_entity_number = 0\n",
    "avg_sentence_len = 0\n",
    "total_sentence = 0\n",
    "total_entity = 0\n",
    "count = 0\n",
    "single_entity = 0\n",
    "# print(path)\n",
    "with open(path)as f_ori:\n",
    "    dic_f_wordlist = json.load(f_ori)\n",
    "    # ff = json.load(f_ori)\n",
    "    # dic_f_wordlist = {}\n",
    "    # for item in ff:\n",
    "    #     # print(item)\n",
    "    #     word_start_end = item['word_start_end']\n",
    "    #     filename = item['filename']\n",
    "    #     if filename not in dic_f_wordlist:\n",
    "    #         dic_f_wordlist[filename] = word_start_end\n",
    "    #     else:\n",
    "    #         dic_f_wordlist[filename].extend(word_start_end)\n",
    "\n",
    "def index_label(test_list):\n",
    "    pre_text = test_list[-1]\n",
    "    newList = []\n",
    "    textList = []\n",
    "    start = 0\n",
    "    for i in range(len(pre_text)):\n",
    "        start =  test_list[i][0]\n",
    "        p_t_s = re.findall(pattern_s,pre_text[i])\n",
    "        for it,t in enumerate(p_t_s):        \n",
    "            end = start + len(t)\n",
    "            if t != ' ':\n",
    "                textList.append(t)\n",
    "                newList.append([start, end])\n",
    "            start = end\n",
    "    newList.append(textList)\n",
    "    return newList\n",
    "\n",
    "def search_entity_index(sentence, ran, d):\n",
    "    list_ner = []\n",
    "    # print(sentence, ran, d)\n",
    "    for i in range(len(d[-2])):\n",
    "        entity_start = d[i][0] - ran[0]\n",
    "        entity_end = d[i][1] - ran[0]\n",
    "        \n",
    "        if i == 0:\n",
    "            # re.findall(pattern,sentence[0:entity_start])\n",
    "            entity_ner = len(re.findall(pattern,sentence[0:entity_start]))\n",
    "            list_ner.append(entity_ner)\n",
    "            \n",
    "        else:\n",
    "            entity_ner = entity_ner + len(re.findall(pattern,sentence[entity_record:entity_start])) + 1\n",
    "            if entity_ner < len(re.findall(pattern,sentence)):\n",
    "                list_ner.append(entity_ner)\n",
    "        entity_record = entity_end\n",
    "    \n",
    "    return list_ner\n",
    "\n",
    "def sentence_ner(diease, sentence_dict):\n",
    "    global single_entity\n",
    "    ans = {}\n",
    "    ans_single = {}\n",
    "    for d in diease:\n",
    "        for k, v in sentence_dict.items():\n",
    "            sentence = v[0]\n",
    "            ran = k\n",
    "            if ran[0]<=d[0][0] and d[-3][-1]<=ran[1]:\n",
    "\n",
    "                list_ner = search_entity_index(sentence, ran, d)        \n",
    "                if ran not in ans_single:     \n",
    "                    ans_single[ran] = []\n",
    "                    ans_single[ran].append({\"index\":list_ner, \"type\":d[-1]})\n",
    "                else:\n",
    "                    ans_single[ran].append({\"index\":list_ner, \"type\":d[-1]})    \n",
    "                single_entity+=1\n",
    "                break\n",
    "    for k, v in sentence_dict.items():\n",
    "        ner = []\n",
    "        for d in diease:\n",
    "            sentence = v[0]\n",
    "            ran = k\n",
    "            if ran[0]<=d[0][0] and d[-3][-1]<=ran[1]:\n",
    "                list_ner = search_entity_index(sentence, ran, d)        \n",
    "                if ran not in ans:\n",
    "                    \n",
    "                    ans[ran] = []\n",
    "                    ans[ran].append({\"index\":list_ner, \"type\":d[-1]})\n",
    "                else:\n",
    "                    ans[ran].append({\"index\":list_ner, \"type\":d[-1]})    \n",
    "        \n",
    "    return ans\n",
    "\n",
    "def merge_intervals(intervals):\n",
    "    if not intervals:\n",
    "        return []\n",
    "\n",
    "    # Sort intervals based on the start value\n",
    "    intervals.sort(key=lambda x: x[0])\n",
    "\n",
    "    merged = [intervals[0]]\n",
    "    \n",
    "    \n",
    "    for current in intervals[1:]:\n",
    "        # Get the last interval in merged\n",
    "        last_interval = merged[-1]\n",
    "\n",
    "    \n",
    "        # if current[0] - last_interval[1] <= merge_intervals_limit and (last_interval[1] - last_interval[0])<entity_intervallimit:\n",
    "        if current[0] - last_interval[1] <= merge_intervals_limit:\n",
    "            # Merge the intervals\n",
    "            merged[-1] = [last_interval[0], max(last_interval[1], current[1])]\n",
    "        else:\n",
    "            # Add the current interval to merged\n",
    "            merged.append(current)\n",
    "\n",
    "    return merged\n",
    "\n",
    "def ans_list(ans, sentence_dict, filename):\n",
    "    global enhance_sentence_length\n",
    "    list_dict = []\n",
    "    count = 0\n",
    "    for k, value in sentence_dict.items():\n",
    "        v = value[0]\n",
    "        span_id = value[1]\n",
    "        start=int(k[0])\n",
    "        end = int(k[1])\n",
    "        word_list = re.findall(pattern_s, v)\n",
    "        # word_index_list = word_index(word_list)\n",
    "        word_index_list = []\n",
    "        for word in word_list:\n",
    "            end = start+len(word)\n",
    "            if word.strip()!=\"\":\n",
    "                word_index_list.append([start, end])\n",
    "            start=end\n",
    "        # print(word_index_list)\n",
    "        json_dict = {\n",
    "            \"sentence\":[], \"ner\":[],\n",
    "            \"filename\": filename, \"word_start_end\":word_index_list,\n",
    "            \"span_id\":span_id\n",
    "        }\n",
    "        json_dict[\"sentence\"] = re.findall(pattern, v)\n",
    "        if k in ans:\n",
    "            json_dict[\"ner\"] = ans[k]\n",
    "            count += len(ans[k])\n",
    "        if json_dict[\"sentence\"] != []:\n",
    "            list_dict.append(json_dict)\n",
    "            # if span_id==1:\n",
    "            enhance_sentence_length.append(len(json_dict[\"sentence\"]))\n",
    "        if len(json_dict[\"sentence\"])==147:\n",
    "            print(filename, json_dict[\"word_start_end\"],json_dict[\"sentence\"])\n",
    "            # print(span_id)\n",
    "\n",
    "    return list_dict, count\n",
    "\n",
    "if is_write:\n",
    "    output_path = output_path\n",
    "else:\n",
    "    output_path = r\"pseudo.json\"\n",
    "\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "    all_list = []\n",
    "    f_name = open(\"answer/\"+file_format+\"/id/\"+data_format+\".id\",\"r\").read().split(\"\\n\")\n",
    "    for filename in f_name:\n",
    "        # filename = \"04649-004477-DISCHARGE_SUMMARY.txt\"\n",
    "        # filename = \"00174-002042-DISCHARGE_SUMMARY.txt\"\n",
    "        # filename = 'LIPITOR.295'\n",
    "        # file_path = folder_path+\"/\"+filename +\".txt\"\n",
    "        if file_format==\"cadec\":\n",
    "            file_path = folder_path+\"/\"+filename +\".txt\"\n",
    "        else:\n",
    "            file_path = folder_path+\"/\"+filename\n",
    "        # print(folder_path)\n",
    "        with open(file_path, encoding='utf-8', errors='ignore') as f:\n",
    "            doc_file = f.read()\n",
    "            # D:\\acl\\data\\share2013\\train\\text\\25844-097135-ECHO_REPORT.txt\n",
    "            if main_block_mode==\"dynamic_size\":\n",
    "                word_number = len(re.findall(pattern,doc_file))\n",
    "                if word_number<=200:\n",
    "                    main_block_size=7\n",
    "                elif 200<word_number<=350:\n",
    "                    main_block_size=9\n",
    "                elif 350<word_number<=500:\n",
    "                    main_block_size=11\n",
    "                elif 500<word_number<=1000:\n",
    "                    main_block_size=13\n",
    "                elif 1000<word_number<=1350:\n",
    "                    main_block_size=15\n",
    "                elif 1350<word_number<=1500:\n",
    "                    main_block_size=16\n",
    "                elif 1500<word_number<=2000:\n",
    "                    main_block_size=17\n",
    "                else:\n",
    "                    main_block_size=19\n",
    "                # origin\n",
    "                # if word_number<=150:\n",
    "                #     main_block_size=4\n",
    "                #     # main_block_size=7\n",
    "                # elif 150<word_number<=350:\n",
    "                #     main_block_size=9\n",
    "                # elif 350<word_number<=500:\n",
    "                #     main_block_size=11\n",
    "                # elif 500<word_number<=1000:\n",
    "                #     main_block_size=13\n",
    "                # elif 1000<word_number<=1350:\n",
    "                #     main_block_size=15\n",
    "                # elif 1350<word_number<=1500:\n",
    "                #     main_block_size=16\n",
    "                # elif 1500<word_number<=2000:\n",
    "                #     main_block_size=17\n",
    "                # else:\n",
    "                #     main_block_size=19\n",
    "            #---------------------------------\n",
    "            #正確答案\n",
    "            try:\n",
    "                if answer_file!=\"\":\n",
    "                    true_answers = ans_tmp.get_group(filename)\n",
    "                    target_start_end = true_answers[['start_end']]\n",
    "                    target_entity = true_answers[['entity']]\n",
    "                    index_to_int = []\n",
    "                    target_entity_list = target_entity.values.tolist()\n",
    "                    target_label = true_answers[['label']].values.tolist()\n",
    "                    for s_e_index, s_e in enumerate(target_start_end.values.tolist()):\n",
    "                        s_e = [int(span) for span in s_e[0].split(',')]\n",
    "                        index_to_int.append([s_e, target_entity_list[s_e_index][0],target_label[s_e_index][0]])\n",
    "                    index_to_int = sorted(index_to_int,key=itemgetter(0))\n",
    "\n",
    "                    items_list = []\n",
    "                    item_s_e = {}\n",
    "                    for item in index_to_int:\n",
    "                        items = []\n",
    "                        # print(item)\n",
    "                        if len(item[0])>2:\n",
    "                            start = 0\n",
    "                            item_str = []\n",
    "                            item_index = []\n",
    "                            itm = item[1]\n",
    "                            for index in range(0, len(item[0]), 2):     \n",
    "                                end = start + (item[0][index+1] - item[0][index])\n",
    "                                item_str.append(itm[start:end])\n",
    "                                itm = itm[end:].strip()\n",
    "                                item_index.append([item[0][index], item[0][index+1]])\n",
    "                            items = item_index+[item_str]\n",
    "                        else:\n",
    "                            items = [item[0], [item[1]]]\n",
    "                        # print(items)\n",
    "                        newList = index_label(items)+[item[2]]\n",
    "                        # print(newList)\n",
    "                        if newList[0][0] not in item_s_e:\n",
    "                            item_s_e[newList[0][0]] = [newList[0][0], newList[-2][-1]]\n",
    "                        else:\n",
    "                            nl = max(item_s_e[newList[0][0]][-1] ,newList[-2][-1])\n",
    "                            item_s_e[newList[0][0]] = [newList[0][0], nl]\n",
    "                        items_list.append(newList)\n",
    "                else:\n",
    "                    items_list=[]\n",
    "            except:\n",
    "                items_list=[]\n",
    "\n",
    "            #---------------------------------\n",
    "            if filename in ans_split_tmp.groups.keys():\n",
    "                split_answers = ans_split_tmp.get_group(filename)\n",
    "            # share14 可以不用\n",
    "            # if filename in ans_split_tmp.groups.keys() or ((data_format==\"train\" or data_format==\"dev\" or data_format==\"test\") and (filename not in ans_split_tmp.groups.keys() and filename in ans_tmp.groups.keys())):\n",
    "            # if filename in ans_split_tmp.groups.keys() or ((data_format==\"train\" or data_format==\"dev\") and (filename not in ans_split_tmp.groups.keys() and filename in ans_tmp.groups.keys())):\n",
    "            #     try:\n",
    "            #         split_answers = ans_split_tmp.get_group(filename)\n",
    "            #     except:\n",
    "            #         split_answers = true_answers\n",
    "            #     # if data_format==\"train\" or data_format==\"dev\" or data_format==\"test\":\n",
    "            #     if data_format==\"train\" or data_format==\"dev\":\n",
    "            #         split_answers['first_start'] = split_answers['start_end'].apply(lambda x: x.split(',')[0])\n",
    "            #         true_answers['first_start'] = true_answers['start_end'].apply(lambda x: x.split(',')[0])\n",
    "            #         # split_answers['first_start'] = split_answers['start_end'].apply(lambda x: \",\".join([x.split(',')[0],x.split(',')[-1]]))\n",
    "            #         # true_answers['first_start'] = true_answers['start_end'].apply(lambda x: \",\".join([x.split(',')[0],x.split(',')[-1]]))\n",
    "            #         miss_data = true_answers.merge(split_answers, on=[\"first_start\"], how='left')\n",
    "            #         miss_data = miss_data[miss_data['file_id_y'].isna()].iloc[:,:6].rename(columns={\n",
    "            #             \"file_id_x\":\"file_id\",\n",
    "            #             \"label_x\":\"label\",\n",
    "            #             \"start_end_x\":\"start_end\",\n",
    "            #             \"entity_x\":\"entity\",\n",
    "            #             \"other_x\":\"other\"\n",
    "            #             })\n",
    "            #         split_answers = split_answers.append(miss_data, ignore_index=True)\n",
    "            #         split_answers = split_answers.drop_duplicates(subset=['first_start'])\n",
    "            #         split_answers.drop(columns=[\"first_start\"],inplace=True)\n",
    "                \n",
    "\n",
    "                split_start_end = split_answers[['start_end']]\n",
    "                split_entity = split_answers[['entity']]\n",
    "                split_index_to_int = []\n",
    "                split_entity_list = split_entity.values.tolist()\n",
    "                for s_e_index, s_e in enumerate(split_start_end.values.tolist()):\n",
    "                    s_e = [int(span) for span in s_e[0].split(',')]\n",
    "                    split_index_to_int.append([s_e, split_entity_list[s_e_index][0]])\n",
    "                split_index_to_int = sorted(split_index_to_int,key=itemgetter(0))\n",
    "            \n",
    "                split_items_list = []\n",
    "                split_item_s_e = {}\n",
    "                for item in split_index_to_int:\n",
    "                    item0 = [[item[0][index], item[0][index+1]] for index in range(0, len(item[0]), 2)]\n",
    "                    item1 = item[1].split(',')\n",
    "                    item0.append(item1)\n",
    "                    if item0[0][0] not in split_item_s_e:\n",
    "                        split_item_s_e[item0[0][0]] = [item0[0][0], item0[-2][-1]]\n",
    "                    else:\n",
    "                        nl = max(split_item_s_e[item0[0][0]][-1] ,item0[-2][-1])\n",
    "                        split_item_s_e[item0[0][0]] = [item0[0][0], nl]\n",
    "                    split_items_list.append(item0)\n",
    "                #------------------------------------------\n",
    "                new_item_s_e = []\n",
    "                for k, v in split_item_s_e.items():\n",
    "                    new_item_s_e.append(v)\n",
    "\n",
    "                # new_item_se = merge_intervals(new_item_s_e)\n",
    "\n",
    "                mergedData = []\n",
    "                start, end = new_item_s_e[0]\n",
    "\n",
    "                for pair in new_item_s_e[1:]:\n",
    "                    if pair[0] <= end:\n",
    "                        end = max(end, pair[1])\n",
    "                    else:\n",
    "                        mergedData.append([start, end])\n",
    "                        start, end = pair\n",
    "\n",
    "                # 加入最後一個合併的範圍\n",
    "                mergedData.append([start, end])\n",
    "                merged_data = merge_intervals(mergedData)\n",
    "                if contain_merge_conflict_span:\n",
    "                    mergedDataArray= np.array(merged_data)\n",
    "                else:\n",
    "                    mergedDataArray= np.array(mergedData)\n",
    "\n",
    "                # --------------------------\n",
    "                # pattern_s = r'[\\w]+|\\n|[\\s]+|[^\\s\\w]'\n",
    "                doc_file_l = list(doc_file)\n",
    "                start = 0\n",
    "                residual_target_dict = {}\n",
    "                for value_indx, value in enumerate(merged_data):\n",
    "                    end = value[0]\n",
    "                    residual = \"\".join(doc_file_l[start:end])\n",
    "                    target = \"\".join(doc_file_l[value[0]:value[1]])\n",
    "                    residual_target_dict[2*value_indx] = residual\n",
    "                    residual_target_dict[2*value_indx+1] = target\n",
    "                    start = value[1]\n",
    "                residual = \"\".join(doc_file_l[value[1]:])\n",
    "                residual_target_dict[len(residual_target_dict)] = residual\n",
    "                \n",
    "                #-------------------------------------------------\n",
    "                tmp = []\n",
    "                finish_list = []\n",
    "                ner = []\n",
    "                total = 0\n",
    "                # total_char = 0\n",
    "                count = 0\n",
    "                ner_count = 0\n",
    "                start = 0\n",
    "                end = 0\n",
    "                pre_start_end = (start, end)\n",
    "                sentence_dict = {}\n",
    "                nest_sentence = []\n",
    "                sentence_history = []\n",
    "                for k,v in residual_target_dict.items():\n",
    "                    residual_v = re.findall(pattern_s,v)\n",
    "                    # print(start,end,k,residual_v)\n",
    "                    if k%2!=0:\n",
    "                        pre_len=total\n",
    "                        pre_s = tmp\n",
    "                        for r_v in residual_v:\n",
    "                            if r_v.strip()!=\"\":\n",
    "                                total += 1\n",
    "                            # total_char += len(r_v)\n",
    "                            count += len(r_v)\n",
    "                            tmp.append(r_v)\n",
    "                        \n",
    "                        end = count\n",
    "                        tmp_value =  \"\".join(tmp)                        \n",
    "                        if sentence_history!=[]:\n",
    "                        \n",
    "                            prev_dict = sentence_dict[(sentence_history[-1][0], sentence_history[-1][1])]\n",
    "\n",
    "                        # if total<=10 and sentence_dict!={} and len(re.findall(pattern,prev_dict[0]))<=main_block_size:\n",
    "                        if sentence_dict!={}:\n",
    "                            tmp_value =  \"\".join(tmp)\n",
    "\n",
    "                            prev_dict[0] = prev_dict[0]+tmp_value\n",
    "                            prev_dict[1] = ES_action_state\n",
    "                            prev_dict[2] = 1\n",
    "                            del sentence_dict[(sentence_history[-1][0], sentence_history[-1][1])]                \n",
    "                            start = sentence_history[-1][0]\n",
    "                            \n",
    "                            sentence_length.pop(-1)\n",
    "                            total_prev_dict = len(re.findall(pattern,prev_dict[0]))\n",
    "\n",
    "                            sentence_dict[(start, start+len(prev_dict[0]))] = prev_dict\n",
    "                            sentence_length.append(len(re.findall(pattern,prev_dict[0])))\n",
    "                            sentence_history.append([start, start+len(prev_dict[0])])\n",
    "\n",
    "                            start = end\n",
    "                            tmp=[]\n",
    "                            total=0\n",
    "                        else:\n",
    "                            # print(start)\n",
    "                            tmp_value =  \"\".join(tmp)\n",
    "                            property = [tmp_value,ES_action_state,1]\n",
    "                            sentence_length.append(total)\n",
    "                            sentence_dict[(start, end)] = property\n",
    "                            sentence_history.append([start, end])\n",
    "                            tmp=[]\n",
    "                            total=0\n",
    "                            start = end\n",
    "\n",
    "                    else:\n",
    "                        # print(residual_v)\n",
    "                        for r_v in residual_v:\n",
    "                            if total >= main_block_size:\n",
    "\n",
    "                                end = count\n",
    "                                tmp_value =  \"\".join(tmp)\n",
    "                                sentence_dict[(start, end)] = [tmp_value,NES_action_state,0]\n",
    "                                sentence_history.append([start, end])\n",
    "                                sentence_length.append(total)\n",
    "                                finish_list.append(sentence_dict)\n",
    "                                start = end                                \n",
    "                                tmp=[]\n",
    "                                total=0\n",
    "                            if r_v.strip()!=\"\":\n",
    "                                total += 1\n",
    "                            # total_char += len(r_v)\n",
    "                            count += len(r_v)\n",
    "                            tmp.append(r_v)\n",
    "                if tmp!=[] and file_format==\"cadec\":\n",
    "                # if tmp!=[] :\n",
    "                    # print(tmp)\n",
    "                    tmp_value =  \"\".join(tmp)\n",
    "                    if tmp_value.strip()!=\"\":\n",
    "                        end = start + len(tmp_value)\n",
    "                #         # print(total)\n",
    "                        sentence_length.append(total)\n",
    "                        sentence_dict[(start, end)] = [tmp_value,NES_action_state,0]\n",
    "\n",
    "                pre = 0\n",
    "                con = 0\n",
    "                new_sentence_dict = {}\n",
    "                check_prev_sentence=\"\"\n",
    "                new_sentence_history=[]\n",
    "                previous_sentence_state = []\n",
    "                forward_step = look_forward_step+1\n",
    "                backward_step = look_backward_step+1\n",
    "                for i, (k, v) in enumerate(sentence_dict.items()):\n",
    "                    sentence_word = re.findall(pattern, v[0])\n",
    "                    sentence_word_num = len(sentence_word)\n",
    "                    if v[1] == 0:\n",
    "                        con += sentence_word_num\n",
    "                        new_sentence_dict[k] = [v[0],v[2]]\n",
    "                        \n",
    "                    else:\n",
    "                        if data_format==\"train\" and add_train_random:\n",
    "                            look_forward_step = random.randint(0,forward_step)\n",
    "                            look_backward_step = random.randint(0,backward_step)\n",
    "\n",
    "                        if support_strategy == 0:\n",
    "                            pre = dic_f_wordlist[filename][max(0, con-look_forward_step)][0]\n",
    "                            # new_sentence_dict[(pre, k[1])] = [doc_file[pre : k[1]],v[2]]\n",
    "                            check_flag = np.logical_and(pre >= mergedDataArray[:, 0], pre < mergedDataArray[:, 1])\n",
    "                            # if pre>4000:\n",
    "                            #     print(check_flag.sum(-1))\n",
    "                            if check_flag.sum(-1)==1:\n",
    "                                conflict_span = mergedDataArray[check_flag][-1]\n",
    "                                # print(conflict_span)\n",
    "                                if conflict_span[0]!=0:\n",
    "                                    # if previous_sentence_state[-1]==0:\n",
    "                                    pre = conflict_span[-1]\n",
    "                            new_sentence_dict[(pre, k[1])] = [doc_file[pre : k[1]],v[2]]\n",
    "\n",
    "                            con += sentence_word_num\n",
    "                            new_sentence_history.append([pre,  k[1]])\n",
    "\n",
    "                        elif support_strategy == 1:\n",
    "\n",
    "                            con += sentence_word_num\n",
    "                            pre = dic_f_wordlist[filename][:con+look_backward_step][-1][-1]\n",
    "                            # if i!=0:\n",
    "                            #     # k[0]=pre\n",
    "                            #     new_start = new_sentence_history[-1][-1]\n",
    "                            # else:\n",
    "                            #     new_start = 0\n",
    "                            check_flag = np.logical_and(pre >= mergedDataArray[:, 0], pre < mergedDataArray[:, 1])\n",
    "                            # if check_flag.sum(-1)==1:\n",
    "                            # if conflict_span[0]!=0:\n",
    "                            if check_flag.sum(-1)==1:\n",
    "                                conflict_span = mergedDataArray[check_flag][0]\n",
    "                                if conflict_span[0]!=0:\n",
    "                                    pre = conflict_span[0]\n",
    "\n",
    "                            new_sentence_dict[(k[0], pre)] = [doc_file[k[0]:pre],v[2]]\n",
    "                            new_sentence_history.append([k[0], pre])\n",
    "\n",
    "                        elif support_strategy == 2:\n",
    "                            \n",
    "                            # if i>0 and len(sentence_dict)>1 and sentence_word_num<7:\n",
    "                            #     continue\n",
    "                            # try:\n",
    "                            # print(look_forward_step)\n",
    "                            if look_forward_step==0:\n",
    "                                pre = k[0]\n",
    "                                # print(pre)\n",
    "       \n",
    "                            else:\n",
    "                                pre = dic_f_wordlist[filename][max(0, con-look_forward_step)][0]\n",
    "                                # print(pre)\n",
    "                            \n",
    "\n",
    "                            check_flag = np.logical_and( pre> mergedDataArray[:, 0], pre <= mergedDataArray[:, 1])\n",
    "                            if check_flag.sum(-1)==1:\n",
    "                                conflict_span = mergedDataArray[check_flag][0]\n",
    "                                if conflict_span[0]!=0:\n",
    "                                    \n",
    "                                    # pre = conflict_span[0]\n",
    "                                    if pre - conflict_span[0]<30:\n",
    "                                        pre = conflict_span[0]\n",
    "                                    else:\n",
    "                                        pre = conflict_span[-1]\n",
    "                            \n",
    "                            con += sentence_word_num\n",
    "                            if look_backward_step==0:\n",
    "                                pre_e = k[-1]\n",
    "                            else:\n",
    "                                pre_e = dic_f_wordlist[filename][:con+look_backward_step][-1][-1]\n",
    "                                check_flag = np.logical_and(pre_e >= mergedDataArray[:, 0], pre_e < mergedDataArray[:, 1])\n",
    "                                if check_flag.sum(-1)==1:\n",
    "                                    conflict_span = mergedDataArray[check_flag][-1]\n",
    "                                    if conflict_span[0]!=0:\n",
    "                                        \n",
    "                                        # pre_e = conflict_span[-1]\n",
    "                                        if conflict_span[-1] - pre_e<30:\n",
    "                                            pre_e = conflict_span[-1]\n",
    "                                        else:\n",
    "                                            pre_e = conflict_span[0]\n",
    "                            \n",
    "                            \n",
    "                            new_sentence_dict[(pre, pre_e)] = [doc_file[pre:pre_e],v[2]]\n",
    "                            new_sentence_history.append([pre, pre_e])\n",
    "                        \n",
    "\n",
    "                    previous_sentence_state = [sentence_word_num,v[1]]\n",
    "                    \n",
    "                    \n",
    "\n",
    "                ans = sentence_ner(items_list, new_sentence_dict)\n",
    "                list_dict, ner_count = ans_list(ans, new_sentence_dict, filename)\n",
    "                total_entity+=ner_count\n",
    "                total_sentence+=len(list_dict)\n",
    "                # if filename in ans_split_tmp.groups.keys() and filename in ans_tmp.groups.keys():\n",
    "                # #     assert ner_count==len(target_entity)\n",
    "                #     if ner_count!=len(target_entity):\n",
    "                #         print(filename)\n",
    "                all_list.extend(list_dict)\n",
    "                \n",
    "            #正確答案(要放ner)/沒預測答案(全切往前看5)\n",
    "            elif filename not in ans_split_tmp.groups.keys():\n",
    "                # print(filename)\n",
    "\n",
    "                sentence_dict = {}\n",
    "                for i in range(0,len(dic_f_wordlist[filename]),main_block_size):\n",
    "                    # start_end= dic_f_wordlist[filename][i:i+main_block_size]\n",
    "                    start_end= dic_f_wordlist[filename][min(i,max(i-look_forward_step,0)):i+main_block_size+look_backward_step]\n",
    "                    if len(start_end)>5: \n",
    "                        # print(start_end)\n",
    "                        sentence = doc_file[start_end[0][0]:start_end[-1][-1]]\n",
    "                        sentence_dict[(start_end[0][0], start_end[-1][-1])] = [sentence,0]\n",
    "                #--------------------------------------------------------------\n",
    "                total_sentence+=len(sentence_dict)\n",
    "                ans = sentence_ner(items_list, sentence_dict)\n",
    "                list_dict, ner_count = ans_list(ans, sentence_dict, filename)\n",
    "                total_entity+=ner_count\n",
    "                # print(ner_count)\n",
    "                # if filename in ans_tmp.groups.keys():\n",
    "                #     assert ner_count==len(target_entity)\n",
    "                all_list.extend(list_dict)\n",
    "        # break\n",
    "    json.dump(all_list, json_file, ensure_ascii=False)\n",
    "import statistics\n",
    "total_sentence,total_entity,single_entity,max(enhance_sentence_length),min(enhance_sentence_length),statistics.mean(enhance_sentence_length),enhance_sentence_length\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "file_format = \"cadec\"\n",
    "ans_data_path=\"test\"\n",
    "ans_split = pd.DataFrame()\n",
    "# folder = \"share13_our_split_2\"\n",
    "folder = \"cadec_19\"\n",
    "# file_list = os.listdir(\"ann/\"+folder+\"/\"+ans_data_path)\n",
    "file_list = ['w2ner_test_0.ann','w2ner_test_1.ann']\n",
    "for enhance_file in file_list:\n",
    "    tmp_df = pd.read_csv(\"ann/\"+folder+\"/\"+ans_data_path+\"/\"+enhance_file, names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "    # tmp_df = pd.read_csv(\"D:/hw/W2NER-main/ann/share14/ann/\"+enhance_file, names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "    # D:\\hw\\W2NER-main\\ann\\share14\\ann\\w2ner_test_123.ann\n",
    "    tmp_df['entity_bound'] = tmp_df['start_end'].apply(lambda x: \",\".join([x.split(',')[0],x.split(',')[-1]]))\n",
    "    tmp_df = tmp_df.drop_duplicates(subset=['file_id','entity_bound'])\n",
    "\n",
    "    ans_split = pd.concat([ans_split,tmp_df])\n",
    "\n",
    "ans_split_df_duplicated = ans_split.duplicated(subset=['file_id','entity_bound'], keep=False)\n",
    "# ans_split = ans_split[ans_split_df_duplicated].groupby(['file_id','entity_bound']).filter(lambda x: len(x) >= (len(file_list))*0.75)\n",
    "ans_split = ans_split[ans_split_df_duplicated].groupby(['file_id','entity_bound']).filter(lambda x: len(x) == (len(file_list)))\n",
    "ans_split = ans_split.drop_duplicates(subset=['file_id','entity_bound'])\n",
    "# ans_split\n",
    "# answer_file='acl/data/'+file_format+'/'+ans_data_path+'.ann'\n",
    "# answer_file='acl/data/cadec/adr/ann'\n",
    "# ans = pd.read_csv(answer_file, names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "# ans['entity_bound'] = ans['start_end'].apply(lambda x: \",\".join([x.split(',')[0],x.split(',')[-1]]))\n",
    "len(ans_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>label</th>\n",
       "      <th>start_end</th>\n",
       "      <th>entity</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARTHROTEC.102</td>\n",
       "      <td>adr</td>\n",
       "      <td>187,190,190,191,191,192,193,198,199,201,202,206</td>\n",
       "      <td>can,',t,stand,or,walk</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARTHROTEC.104</td>\n",
       "      <td>adr</td>\n",
       "      <td>5,13</td>\n",
       "      <td>diarrhea</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARTHROTEC.104</td>\n",
       "      <td>adr</td>\n",
       "      <td>20,32</td>\n",
       "      <td>constipation</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARTHROTEC.104</td>\n",
       "      <td>adr</td>\n",
       "      <td>34,41</td>\n",
       "      <td>fatigue</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARTHROTEC.104</td>\n",
       "      <td>adr</td>\n",
       "      <td>161,167,168,182,183,185,186,189,190,195</td>\n",
       "      <td>severe,osteoarthritis,in,the,knees</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>VOLTAREN.8</td>\n",
       "      <td>adr</td>\n",
       "      <td>392,402</td>\n",
       "      <td>hoarseness</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>VOLTAREN.8</td>\n",
       "      <td>adr</td>\n",
       "      <td>403,411</td>\n",
       "      <td>coughing</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>VOLTAREN.8</td>\n",
       "      <td>adr</td>\n",
       "      <td>413,418,419,425</td>\n",
       "      <td>heart,racing</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>VOLTAREN.8</td>\n",
       "      <td>adr</td>\n",
       "      <td>427,438</td>\n",
       "      <td>disoriented</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>ZIPSOR.5</td>\n",
       "      <td>adr</td>\n",
       "      <td>540,544</td>\n",
       "      <td>pain</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            file_id label                                        start_end  \\\n",
       "0     ARTHROTEC.102   adr  187,190,190,191,191,192,193,198,199,201,202,206   \n",
       "1     ARTHROTEC.104   adr                                             5,13   \n",
       "2     ARTHROTEC.104   adr                                            20,32   \n",
       "3     ARTHROTEC.104   adr                                            34,41   \n",
       "4     ARTHROTEC.104   adr          161,167,168,182,183,185,186,189,190,195   \n",
       "...             ...   ...                                              ...   \n",
       "1005     VOLTAREN.8   adr                                          392,402   \n",
       "1006     VOLTAREN.8   adr                                          403,411   \n",
       "1007     VOLTAREN.8   adr                                  413,418,419,425   \n",
       "1008     VOLTAREN.8   adr                                          427,438   \n",
       "1009       ZIPSOR.5   adr                                          540,544   \n",
       "\n",
       "                                  entity other  \n",
       "0                  can,',t,stand,or,walk        \n",
       "1                               diarrhea        \n",
       "2                           constipation        \n",
       "3                                fatigue        \n",
       "4     severe,osteoarthritis,in,the,knees        \n",
       "...                                  ...   ...  \n",
       "1005                          hoarseness        \n",
       "1006                            coughing        \n",
       "1007                        heart,racing        \n",
       "1008                         disoriented        \n",
       "1009                                pain        \n",
       "\n",
       "[1010 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_df = pd.read_csv(\"ann/\"+folder+\"/\"+ans_data_path+\"/\"+enhance_file, names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.merge(ans_split,ans,how=\"inner\",on=['file_id','entity_bound'])\n",
    "# .iloc[:,:6].rename(columns={\"label_x\":\"label\",\"start_end_x\":\"start_end\",\"entity_x\":\"entity\"})\n",
    "# ans\n",
    "x = pd.merge(ans_split,ans,how=\"left\",on=['file_id','entity_bound'])\n",
    "w = x[x['label_y'].isna()].iloc[:,:5].rename(columns={\"label_x\":\"label\",\"start_end_x\":\"start_end\",\"entity_x\":\"entity\"})\n",
    "len(w)\n",
    "# w.to_csv(\"error_predict.ann\", sep='\\t', index=False,header=None)\n",
    "# ans_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_1 = pd.read_csv(\"ann/\"+folder+\"/test/w2ner_test_0.ann\", names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "# p_2 = pd.read_csv(\"ann/\"+folder+\"/test/w2ner_test_1.ann\", names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "# p_3 = pd.read_csv(\"ann/\"+folder+\"/test/w2ner_test_2.ann\", names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "# p_4 = pd.read_csv(\"ann/\"+folder+\"/test/w2ner_test_3.ann\", names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "# p_5 = pd.read_csv(\"ann/\"+folder+\"/test/w2ner_test_4.ann\", names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "# p_c = pd.concat([p_1,p_2,p_3,p_4])\n",
    "p_i = pd.merge(p_1,p_2,how=\"inner\")\n",
    "# p_i = pd.merge(p_i,p_3,how=\"inner\")\n",
    "# p_i = pd.merge(p_i,p_4,how=\"inner\")\n",
    "# p_i = pd.merge(p_i,p_5,how=\"inner\")\n",
    "p_i['entity_bound'] = p_i['start_end'].apply(lambda x: \",\".join([x.split(',')[0],x.split(',')[-1]]))\n",
    "# ans_split = p_c.drop_duplicates(subset=['file_id','start_end'])\n",
    "# ans_split_tmp = p_i.groupby(['file_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>label</th>\n",
       "      <th>start_end</th>\n",
       "      <th>entity</th>\n",
       "      <th>other_x</th>\n",
       "      <th>entity_bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00176-102920-ECHO_REPORT.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>120,131,132,140</td>\n",
       "      <td>Pericardial,effusion</td>\n",
       "      <td></td>\n",
       "      <td>120,140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00176-102920-ECHO_REPORT.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>430,434,435,441,442,444,445,455,456,463</td>\n",
       "      <td>left,atrium,is,moderately,dilated</td>\n",
       "      <td></td>\n",
       "      <td>430,463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00176-102920-ECHO_REPORT.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>534,539,540,546,547,549,550,560,561,568</td>\n",
       "      <td>right,atrium,is,moderately,dilated</td>\n",
       "      <td></td>\n",
       "      <td>534,568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00176-102920-ECHO_REPORT.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>614,618,619,630,631,642</td>\n",
       "      <td>left,ventricular,hypertrophy</td>\n",
       "      <td></td>\n",
       "      <td>614,642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00176-102920-ECHO_REPORT.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>809,813,814,825,826,830,831,837,838,851</td>\n",
       "      <td>left,ventricular,wall,motion,abnormalities</td>\n",
       "      <td></td>\n",
       "      <td>809,851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4119</th>\n",
       "      <td>26522-011368-DISCHARGE_SUMMARY.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>6661,6671</td>\n",
       "      <td>hemorrhage</td>\n",
       "      <td></td>\n",
       "      <td>6661,6671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>26522-011368-DISCHARGE_SUMMARY.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>6849,6856,6857,6859</td>\n",
       "      <td>dropped,BP</td>\n",
       "      <td></td>\n",
       "      <td>6849,6859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4121</th>\n",
       "      <td>26522-011368-DISCHARGE_SUMMARY.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>7428,7436,7437,7439</td>\n",
       "      <td>dropping,BP</td>\n",
       "      <td></td>\n",
       "      <td>7428,7439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4122</th>\n",
       "      <td>26522-011368-DISCHARGE_SUMMARY.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>7452,7459,7460,7470</td>\n",
       "      <td>cardiac,arrhythmia</td>\n",
       "      <td></td>\n",
       "      <td>7452,7470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4123</th>\n",
       "      <td>26522-011368-DISCHARGE_SUMMARY.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>7990,8001,8002,8009</td>\n",
       "      <td>respiratory,failure</td>\n",
       "      <td></td>\n",
       "      <td>7990,8009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4124 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 file_id     label  \\\n",
       "0           00176-102920-ECHO_REPORT.txt  disorder   \n",
       "1           00176-102920-ECHO_REPORT.txt  disorder   \n",
       "2           00176-102920-ECHO_REPORT.txt  disorder   \n",
       "3           00176-102920-ECHO_REPORT.txt  disorder   \n",
       "4           00176-102920-ECHO_REPORT.txt  disorder   \n",
       "...                                  ...       ...   \n",
       "4119  26522-011368-DISCHARGE_SUMMARY.txt  disorder   \n",
       "4120  26522-011368-DISCHARGE_SUMMARY.txt  disorder   \n",
       "4121  26522-011368-DISCHARGE_SUMMARY.txt  disorder   \n",
       "4122  26522-011368-DISCHARGE_SUMMARY.txt  disorder   \n",
       "4123  26522-011368-DISCHARGE_SUMMARY.txt  disorder   \n",
       "\n",
       "                                    start_end  \\\n",
       "0                             120,131,132,140   \n",
       "1     430,434,435,441,442,444,445,455,456,463   \n",
       "2     534,539,540,546,547,549,550,560,561,568   \n",
       "3                     614,618,619,630,631,642   \n",
       "4     809,813,814,825,826,830,831,837,838,851   \n",
       "...                                       ...   \n",
       "4119                                6661,6671   \n",
       "4120                      6849,6856,6857,6859   \n",
       "4121                      7428,7436,7437,7439   \n",
       "4122                      7452,7459,7460,7470   \n",
       "4123                      7990,8001,8002,8009   \n",
       "\n",
       "                                          entity other_x entity_bound  \n",
       "0                           Pericardial,effusion              120,140  \n",
       "1              left,atrium,is,moderately,dilated              430,463  \n",
       "2             right,atrium,is,moderately,dilated              534,568  \n",
       "3                   left,ventricular,hypertrophy              614,642  \n",
       "4     left,ventricular,wall,motion,abnormalities              809,851  \n",
       "...                                          ...     ...          ...  \n",
       "4119                                  hemorrhage            6661,6671  \n",
       "4120                                  dropped,BP            6849,6859  \n",
       "4121                                 dropping,BP            7428,7439  \n",
       "4122                          cardiac,arrhythmia            7452,7470  \n",
       "4123                         respiratory,failure            7990,8009  \n",
       "\n",
       "[4124 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(p_i,ans,how=\"inner\",on=['file_id','entity_bound']).iloc[:,:6].rename(columns={\"label_x\":\"label\",\"start_end_x\":\"start_end\",\"entity_x\":\"entity\"})\n",
    "# p_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Terrible short term memory problems, lack of motivation, weak and wobbly unbalanced feeling (walked like a 90 year old), not sure footed as I walked, overall weak feeling, difficulty driving due to muscle function irregularities, tightness in throat with occasional choking problem, lightheaded foggy brain (like being in a stupor), loss of endurance, heavy legs and arms, eye muscle spasms, dizziness (vertigo), lousy sleeping at night, terrible concentration, bad dreams nightly, foot and leg cramps especially at night in bed, always exhausted, hip joint pain, muscular aches, swallowing problems, hard to get out of bed in the morning, constant muscle tension in legs (inflexibility), sadness, mood swings, loss of muscle mass in chest and arms.\\nI took 10 mg/day of Lipitor for 6 years.\\nIt worked perfectly and made my blood test numbers for cholesterol, HDL/LDL and Triglycerides look textbook perfect.\\nA few of the symptoms I listed (mostly muscular) quickly appeared when I began the Lipitor and became slowly more intense and more of them came into the picture as each year passed.\\nI stopped the Lipitor recently out of desperation and most all symptoms improved greatly within days of stopping.\\nI do not plan to resume it and will find some other way to control things.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: 'Terrible short term memory problems, lack of motivation, weak and wobbly unbalanced feeling (walked like a 90 year old), not sure footed as I walked, overall weak feeling, difficulty driving due to muscle function irregularities, tightness in throat with occasional choking problem, lightheaded foggy brain (like being in a stupor), loss of endurance, heavy legs and arms, eye muscle spasms, dizziness (vertigo), lousy sleeping at night, terrible concentration, bad dreams nightly, foot and leg cramps especially at night in bed, always exhausted, hip joint pain, muscular aches, swallowing problems, hard to get out of bed in the morning, constant muscle tension in legs (inflexibility), sadness, mood swings, loss of muscle mass in chest and arms',\n",
       " 2: '.\\nI took 10 mg/day of Lipitor for 6 years.\\nIt worked perfectly and made my blood test numbers for cholesterol, HDL/LDL and Triglycerides look textbook perfect.\\nA few of the symptoms I listed (mostly muscular) quickly appeared when I began the Lipitor and became slowly more intense and more of them came into the picture as each year passed.\\nI stopped the Lipitor recently out of desperation and most all symptoms improved greatly within days of stopping.\\nI do not plan to resume it and will find some other way to control things.\\n'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>label</th>\n",
       "      <th>start_end</th>\n",
       "      <th>entity</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>diease</td>\n",
       "      <td>0,8,9,14,15,19,20,26,27,35</td>\n",
       "      <td>Terrible,short,term,memory,problems</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>diease</td>\n",
       "      <td>9,14,15,19,20,26,27,35</td>\n",
       "      <td>short,term,memory,problems</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>diease</td>\n",
       "      <td>37,41,42,44,45,55,55,56,57,61,62,65,66,72,73,8...</td>\n",
       "      <td>lack,of,motivation,,,weak,and,wobbly,unbalance...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>diease</td>\n",
       "      <td>37,41,42,44,45,55,55,56,57,61,62,65,66,72,73,8...</td>\n",
       "      <td>lack,of,motivation,,,weak,and,wobbly,unbalance...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>diease</td>\n",
       "      <td>37,41,42,44,45,55,55,56,57,61,62,65,66,72,73,8...</td>\n",
       "      <td>lack,of,motivation,,,weak,and,wobbly,unbalance...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>diease</td>\n",
       "      <td>640,648,649,655,656,663,664,666,667,671,672,67...</td>\n",
       "      <td>constant,muscle,tension,in,legs,(,inflexibility,)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>diease</td>\n",
       "      <td>689,696</td>\n",
       "      <td>sadness</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>diease</td>\n",
       "      <td>698,702,703,709</td>\n",
       "      <td>mood,swings</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>diease</td>\n",
       "      <td>711,715,716,718,719,725,726,730,731,733,734,73...</td>\n",
       "      <td>loss,of,muscle,mass,in,chest,and,arms</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>diease</td>\n",
       "      <td>711,715,716,718,719,725,726,730,731,733,734,739</td>\n",
       "      <td>loss,of,muscle,mass,in,chest</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_id   label                                          start_end  \\\n",
       "286  LIPITOR.295  diease                         0,8,9,14,15,19,20,26,27,35   \n",
       "287  LIPITOR.295  diease                             9,14,15,19,20,26,27,35   \n",
       "288  LIPITOR.295  diease  37,41,42,44,45,55,55,56,57,61,62,65,66,72,73,8...   \n",
       "289  LIPITOR.295  diease  37,41,42,44,45,55,55,56,57,61,62,65,66,72,73,8...   \n",
       "290  LIPITOR.295  diease  37,41,42,44,45,55,55,56,57,61,62,65,66,72,73,8...   \n",
       "..           ...     ...                                                ...   \n",
       "352  LIPITOR.295  diease  640,648,649,655,656,663,664,666,667,671,672,67...   \n",
       "353  LIPITOR.295  diease                                            689,696   \n",
       "354  LIPITOR.295  diease                                    698,702,703,709   \n",
       "355  LIPITOR.295  diease  711,715,716,718,719,725,726,730,731,733,734,73...   \n",
       "356  LIPITOR.295  diease    711,715,716,718,719,725,726,730,731,733,734,739   \n",
       "\n",
       "                                                entity other  \n",
       "286                Terrible,short,term,memory,problems        \n",
       "287                         short,term,memory,problems        \n",
       "288  lack,of,motivation,,,weak,and,wobbly,unbalance...        \n",
       "289  lack,of,motivation,,,weak,and,wobbly,unbalance...        \n",
       "290  lack,of,motivation,,,weak,and,wobbly,unbalance...        \n",
       "..                                                 ...   ...  \n",
       "352  constant,muscle,tension,in,legs,(,inflexibility,)        \n",
       "353                                            sadness        \n",
       "354                                        mood,swings        \n",
       "355              loss,of,muscle,mass,in,chest,and,arms        \n",
       "356                       loss,of,muscle,mass,in,chest        \n",
       "\n",
       "[71 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>label</th>\n",
       "      <th>start_end</th>\n",
       "      <th>entity</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>0,35</td>\n",
       "      <td>Terrible short term memory problems</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>37,55</td>\n",
       "      <td>lack of motivation</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>57,83</td>\n",
       "      <td>weak and wobbly unbalanced</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>93,118</td>\n",
       "      <td>walked like a 90 year old</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>121,148</td>\n",
       "      <td>not sure footed as I walked</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>150,170</td>\n",
       "      <td>overall weak feeling</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>198,228</td>\n",
       "      <td>muscle function irregularities</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>230,281</td>\n",
       "      <td>tightness in throat with occasional choking pr...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>283,331</td>\n",
       "      <td>lightheaded foggy brain (like being in a stupor)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>333,350</td>\n",
       "      <td>loss of endurance</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>352,371</td>\n",
       "      <td>heavy legs and arms</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>373,390</td>\n",
       "      <td>eye muscle spasms</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>392,401</td>\n",
       "      <td>dizziness</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>403,410</td>\n",
       "      <td>vertigo</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>413,436</td>\n",
       "      <td>lousy sleeping at night</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>438,460</td>\n",
       "      <td>terrible concentration</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>462,480</td>\n",
       "      <td>bad dreams nightly</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>482,501</td>\n",
       "      <td>foot and leg cramps</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>530,546</td>\n",
       "      <td>always exhausted</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>548,562</td>\n",
       "      <td>hip joint pain</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>564,578</td>\n",
       "      <td>muscular aches</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>580,599</td>\n",
       "      <td>swallowing problems</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>601,638</td>\n",
       "      <td>hard to get out of bed in the morning</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>640,671</td>\n",
       "      <td>constant muscle tension in legs</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>673,686</td>\n",
       "      <td>inflexibility</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>689,696</td>\n",
       "      <td>sadness</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>698,709</td>\n",
       "      <td>mood swings</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>711,733,744,748</td>\n",
       "      <td>loss of muscle mass in arms</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>LIPITOR.295</td>\n",
       "      <td>ADR</td>\n",
       "      <td>711,733,734,739</td>\n",
       "      <td>loss of muscle mass in chest</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_id label        start_end  \\\n",
       "1963  LIPITOR.295   ADR             0,35   \n",
       "1964  LIPITOR.295   ADR            37,55   \n",
       "1965  LIPITOR.295   ADR            57,83   \n",
       "1966  LIPITOR.295   ADR           93,118   \n",
       "1967  LIPITOR.295   ADR          121,148   \n",
       "1968  LIPITOR.295   ADR          150,170   \n",
       "1969  LIPITOR.295   ADR          198,228   \n",
       "1970  LIPITOR.295   ADR          230,281   \n",
       "1971  LIPITOR.295   ADR          283,331   \n",
       "1972  LIPITOR.295   ADR          333,350   \n",
       "1973  LIPITOR.295   ADR          352,371   \n",
       "1974  LIPITOR.295   ADR          373,390   \n",
       "1975  LIPITOR.295   ADR          392,401   \n",
       "1976  LIPITOR.295   ADR          403,410   \n",
       "1977  LIPITOR.295   ADR          413,436   \n",
       "1978  LIPITOR.295   ADR          438,460   \n",
       "1979  LIPITOR.295   ADR          462,480   \n",
       "1980  LIPITOR.295   ADR          482,501   \n",
       "1981  LIPITOR.295   ADR          530,546   \n",
       "1982  LIPITOR.295   ADR          548,562   \n",
       "1983  LIPITOR.295   ADR          564,578   \n",
       "1984  LIPITOR.295   ADR          580,599   \n",
       "1985  LIPITOR.295   ADR          601,638   \n",
       "1986  LIPITOR.295   ADR          640,671   \n",
       "1987  LIPITOR.295   ADR          673,686   \n",
       "1988  LIPITOR.295   ADR          689,696   \n",
       "1989  LIPITOR.295   ADR          698,709   \n",
       "1990  LIPITOR.295   ADR  711,733,744,748   \n",
       "1991  LIPITOR.295   ADR  711,733,734,739   \n",
       "\n",
       "                                                 entity other  \n",
       "1963                Terrible short term memory problems        \n",
       "1964                                 lack of motivation        \n",
       "1965                         weak and wobbly unbalanced        \n",
       "1966                          walked like a 90 year old        \n",
       "1967                        not sure footed as I walked        \n",
       "1968                               overall weak feeling        \n",
       "1969                     muscle function irregularities        \n",
       "1970  tightness in throat with occasional choking pr...        \n",
       "1971   lightheaded foggy brain (like being in a stupor)        \n",
       "1972                                  loss of endurance        \n",
       "1973                                heavy legs and arms        \n",
       "1974                                  eye muscle spasms        \n",
       "1975                                          dizziness        \n",
       "1976                                            vertigo        \n",
       "1977                            lousy sleeping at night        \n",
       "1978                             terrible concentration        \n",
       "1979                                 bad dreams nightly        \n",
       "1980                                foot and leg cramps        \n",
       "1981                                   always exhausted        \n",
       "1982                                     hip joint pain        \n",
       "1983                                     muscular aches        \n",
       "1984                                swallowing problems        \n",
       "1985              hard to get out of bed in the morning        \n",
       "1986                    constant muscle tension in legs        \n",
       "1987                                      inflexibility        \n",
       "1988                                            sadness        \n",
       "1989                                        mood swings        \n",
       "1990                        loss of muscle mass in arms        \n",
       "1991                       loss of muscle mass in chest        "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>label</th>\n",
       "      <th>start_end</th>\n",
       "      <th>entity</th>\n",
       "      <th>other</th>\n",
       "      <th>entity_bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>06828-026269-DISCHARGE_SUMMARY.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>2354,2361,2370,2374,2375,2379</td>\n",
       "      <td>Chronic,back,pain</td>\n",
       "      <td></td>\n",
       "      <td>2354,2379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>06828-026269-DISCHARGE_SUMMARY.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>2354,2361,2362,2369,2375,2379</td>\n",
       "      <td>Chronic,buttock,pain</td>\n",
       "      <td></td>\n",
       "      <td>2354,2379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5713</th>\n",
       "      <td>16737-003522-DISCHARGE_SUMMARY.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>6257,6261,6268,6274,6275,6283</td>\n",
       "      <td>open,fibula,fracture</td>\n",
       "      <td></td>\n",
       "      <td>6257,6283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5714</th>\n",
       "      <td>16737-003522-DISCHARGE_SUMMARY.txt</td>\n",
       "      <td>disorder</td>\n",
       "      <td>6257,6261,6262,6267,6275,6283</td>\n",
       "      <td>open,tibia,fracture</td>\n",
       "      <td></td>\n",
       "      <td>6257,6283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 file_id     label  \\\n",
       "1933  06828-026269-DISCHARGE_SUMMARY.txt  disorder   \n",
       "1934  06828-026269-DISCHARGE_SUMMARY.txt  disorder   \n",
       "5713  16737-003522-DISCHARGE_SUMMARY.txt  disorder   \n",
       "5714  16737-003522-DISCHARGE_SUMMARY.txt  disorder   \n",
       "\n",
       "                          start_end                entity other entity_bound  \n",
       "1933  2354,2361,2370,2374,2375,2379     Chronic,back,pain          2354,2379  \n",
       "1934  2354,2361,2362,2369,2375,2379  Chronic,buttock,pain          2354,2379  \n",
       "5713  6257,6261,6268,6274,6275,6283  open,fibula,fracture          6257,6283  \n",
       "5714  6257,6261,6262,6267,6275,6283   open,tibia,fracture          6257,6283  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "ans_split = pd.DataFrame()\n",
    "# file_list = os.listdir(\"/data/m11115088/W2NER_final/ann/share14/\"+data_format)\n",
    "file_list = os.listdir(\"ann/share14_our_split_1_new_inner_look_both_side_ES-dynamic_size-F3-B3/test\")\n",
    "for enhance_file in file_list:\n",
    "    tmp_df = pd.read_csv(\"ann/share14_our_split_1_new_inner_look_both_side_ES-dynamic_size-F3-B3/test/\"+enhance_file, names=[\"file_id\", \"label\", \"start_end\", \"entity\", \"other\"], keep_default_na=False, na_values=['NaN', 'null'], delimiter='\\t')\n",
    "    tmp_df['entity_bound'] = tmp_df['start_end'].apply(lambda x: \",\".join([x.split(',')[0],x.split(',')[-1]]))\n",
    "    ans_split = pd.concat([ans_split,tmp_df])\n",
    "\n",
    "ans_split_df_duplicated = ans_split.duplicated(subset=['file_id','entity_bound'], keep=False)\n",
    "# ans_split = ans_split[ans_split_df_duplicated].groupby(['file_id','entity_bound']).filter(lambda x: len(x) >= (len(file_list))*0.75)\n",
    "# ans_split = ans_split.drop_duplicates(subset=['file_id','entity_bound'])\n",
    "ans_split[ans_split_df_duplicated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0  : total sentence:   1 total entity: 2.000000'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:3.4s}: total sentence: {:3.0f} total entity: {:3f}\".format(\"0\",1,2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
